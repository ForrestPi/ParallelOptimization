

## 深度学习程序性能优化技术

随着深度学习的发展，用户越来越依赖 GPU 或者其他加速器进行大规模运算。人工智能（Artificial Intelligence）需要更优秀的软件来释放硬件的能量已成业界共识。一方面，各种框架需要进一步降低编写深度学习分布式训练程序的门槛；另一方面，用户期待系统可以支持不同的深度学习网络模型，并实现线性加速。各知名深度学习框架正在朝这方面努力，但用户在使用这些框架时仍会遇到横向扩展性的难题，或者是投入很多计算资源但没有看到效率收益，或者是问题规模超过 GPU 显存限制而无法求解。我们团队历时两年研发了一套全新的深度学习引擎 OneFlow，在大数据和大模型场景都展现了相对于已有引擎的显著优势。业界关于这方面的技术教程还不够系统，我们希望通过这个课程能把我们研发 OneFlow 过程中的经验教训及关键技术分享给更多人，这些关键技术并不仅限于 OneFlow，也同样适用于其它深度学习框架的性能优化。

课程收益：使用多机多卡训练深度学习模型的最优实践，使用已有框架遇到性能问题时知道如何去调优，重新写一个深度学习系统需要哪些关键技术。

课程大纲：

    微观层面，CUDA 编程精要，GPU 硬件架构，如何写一个高效的 cuda kernel
    如何高效的利用 GPU? 如何用好 CUDA，Stream，Event重叠计算和传输
    如何使用 CUBLAS，Cudnn，NCCL 编程
    RDMA 原理和实践
    如何优化数据流水线？
    异构计算的内存管理技术
    数据并行，模型并行和流水并行的原理
    如何优化多机多卡代码？
    我眼里深度学习引擎的最优架构

